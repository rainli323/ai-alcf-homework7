Model:  BertLarge
Date:  04/08/24
Time:  23:33

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

Listing...
sambaflow/focal,focal,now 1.17.7-27 amd64 [installed,upgradable to: 1.18.7-38]
Machine State Before: 
Platform: DataScale SN30-8

Physical Inventory:
Component Name                        | Serial Number       | Inventory State | Functional State
------------------------------------------------------------------------------------------------
/NODE/XRDU_0/RDU_0                    | 205057B469B35895    | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_0/DIMM_A0    | 1F310A0             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_1/DIMM_B0    | 1F31085             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_2/DIMM_E0    | 1F3123D             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_3/DIMM_F0    | 1F310E6             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_4/DIMM_G0    | 1F6F66D             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_5/DIMM_H0    | 1F6F8AE             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_6/DIMM_C0    | 1F31058             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_7/DIMM_D0    | 1F31185             | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1                    | 60504FB469B35895    | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_0/DIMM_J0    | 1F31211             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_1/DIMM_K0    | 1F312E9             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_2/DIMM_N0    | 1F312F6             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_3/DIMM_P0    | 1F31160             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_4/DIMM_Q0    | 1F3129B             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_5/DIMM_R0    | 1F31103             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_6/DIMM_L0    | 1F312FC             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_7/DIMM_M0    | 1F3124F             | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0                    | 400804356D2D5895    | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_0/DIMM_A0    | 1F5BB53             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_1/DIMM_B0    | 1F5BD97             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_2/DIMM_E0    | 1F5BB36             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_3/DIMM_F0    | 1F5BB2D             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_4/DIMM_G0    | 1F5BD6E             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_5/DIMM_H0    | 1F5BB50             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_6/DIMM_C0    | 1F5BACD             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_7/DIMM_D0    | 1F5BDEC             | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1                    | 605030356D2D5895    | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_0/DIMM_J0    | 1F5BB47             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_1/DIMM_K0    | 1F5BB41             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_2/DIMM_N0    | 1F5BB72             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_3/DIMM_P0    | 1F5BBDC             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_4/DIMM_Q0    | 1F5BB8B             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_5/DIMM_R0    | 1F5BCF1             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_6/DIMM_L0    | 1F5BB6C             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_7/DIMM_M0    | 1F5BD33             | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0                    | 507076B16ABDB895    | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_0/DIMM_A0    | 1F5BD93             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_1/DIMM_B0    | 1F5BB10             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_2/DIMM_E0    | 1F5BB0F             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_3/DIMM_F0    | 1F5BD9A             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_4/DIMM_G0    | 1F5BD8D             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_5/DIMM_H0    | 1F5BD46             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_6/DIMM_C0    | 1F5BB1A             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_7/DIMM_D0    | 1F5BB0E             | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1                    | 30702AB16ABDB895    | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_0/DIMM_J0    | 1F5BDC7             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_1/DIMM_K0    | 1F5BAF9             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_2/DIMM_N0    | 1F5BB08             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_3/DIMM_P0    | 1F5BDBA             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_4/DIMM_Q0    | 1F5BD99             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_5/DIMM_R0    | 1F5BB15             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_6/DIMM_L0    | 1F5BB2C             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_7/DIMM_M0    | 1F5BB2B             | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0                    | 702867316ABDB895    | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_0/DIMM_A0    | 1F5BBE4             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_1/DIMM_B0    | 1F5BC28             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_2/DIMM_E0    | 1F5BCC6             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_3/DIMM_F0    | 1F5BDF2             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_4/DIMM_G0    | 1F5BAFE             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_5/DIMM_H0    | 1F5BAEC             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_6/DIMM_C0    | 1F5BBA7             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_7/DIMM_D0    | 1F5BC96             | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1                    | 60084B316ABDB895    | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_0/DIMM_J0    | 1F5BDBE             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_1/DIMM_K0    | 1F5BC0D             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_2/DIMM_N0    | 1F5BC99             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_3/DIMM_P0    | 1F5BB68             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_4/DIMM_Q0    | 1F5BC21             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_5/DIMM_R0    | 1F5BC22             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_6/DIMM_L0    | 1F5BC9C             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_7/DIMM_M0    | 1F5BC17             | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/HOST/HIC_0/DPORT                | N/A                 | Present         | Online       
/NODE/HOST/HIC_1/DPORT                | N/A                 | Present         | Online       
/NODE/HOST/HIC_2/DPORT                | N/A                 | Present         | Online       
/NODE/HOST/HIC_3/DPORT                | N/A                 | Present         | Online
COMPILE START AT 0
COMPILE COMMAND: python /opt/sambaflow/apps/nlp/transformers_on_rdu/transformers_hook.py compile --model_name_or_path bert-large-uncased --tokenizer_name bert-large-uncased --module_name mlm_ns --task_name mlm_ns  --max_seq_length 128 --per_device_train_batch_size 256 -b 256 --output_dir=/home/rainli/BertLarge/hf_output --overwrite_output_dir --cache_dir /home/rainli/BertLarge/cache --compiler-configs-file /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions/compiler_configs/compiler_configs_bertlarge_sc_mlm_ml_perf_fullfeature_macv2_gm.json --mac-human-decision /opt/sambaflow/apps/nlp/transformers_on_rdu/human_decisions/mac_overrides/bertlarge_sc_training_mlm_ml_perf_fullfeature_macv2.json --mac-v2 --non_split_head --dense_adam --data-parallel -ws 2 --weight_decay 0.01 --max_grad_norm_clip 1.0 --adam_beta2 0.98 --num-tiles 4 --pef-name=bertlrg --output-folder=/home/rainli/BertLarge --log-level error --disable-strict-conversion
PyTorch version 1.10.2+cpu available.
TensorFlow version 2.7.0 available.
/usr/local/lib/python3.8/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dtype=np.int):
2024-04-08 23:33:22,138 - apps.nlp.transformers_on_rdu.transformers_hook - Process ID 1688339 - info     - NLP app started.
2024-04-08 23:33:22,238 - apps.nlp.transformers_on_rdu.tasks.utils.lazy_gpt2_pretrain - Process ID 1688339 - info     - Patching gpt2 hf model attr 'GPT2PreTrainedModel._init_weights'
2024-04-08 23:33:22,238 - apps.nlp.transformers_on_rdu.tasks.utils.lazy_gpt2_pretrain - Process ID 1688339 - info     - Patching mlm_ns hf model attr 'BertPreTrainedModel._init_weights'
Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['cls.predictions.decoder.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-04-08 23:33:22,930 - apps.nlp.transformers_on_rdu.tasks.utils.lazy_gpt2_pretrain - Process ID 1688339 - warning  - This module is not supported under lazy param initialization: tasks.lm_tasks.bert_mlperf_lm.BertForMaskedLM.BertLMPredictionHead
2024-04-08 23:33:23,514 - apps.nlp.transformers_on_rdu.transformers_hook - Process ID 1688339 - info     - Dataset is loading.
2024-04-08 23:33:23,514 - apps.nlp.transformers_on_rdu.transformers_hook - Process ID 1688339 - info     - Dataset has finished loading.
2024-04-08 23:33:23,515 - apps.nlp.transformers_on_rdu.transformers_hook - Process ID 1688339 - info     - transformers_hook app running in compile mode
Log ID initialized to: [rainli][python][1688339] at /var/log/sambaflow/runtime/sn.log
2024-04-08 23:33:23,911 - py.warnings - Process ID 1688339 - warning  - /usr/local/lib/python3.8/site-packages/torch/overrides.py:1349: DeprecationWarning: Defining your `__torch_function__ as a plain method is deprecated and will be an error in PyTorch 1.11, please define it as a classmethod.
  warnings.warn("Defining your `__torch_function__ as a plain method is deprecated and "

[info    ]: section boundary name: fwd_section_boundaries
[info    ]: section type 1: 
[info    ]: section boundary name: bwd_section_boundaries
[info    ]: section type 1: 
[info    ]: section boundary name: opt_section_boundaries
[info    ]: section type 1: 
[info    ]: section boundary name: fwd_naming_boundaries
[info    ]: section type 1: 
[info    ]: section boundary name: bwd_naming_boundaries
[info    ]: section type 1: 
[info    ]: Using pymac workaround, GraphAMP pass will not run.
[info    ]: Using pymac workaround, GraphAMP Legalizer pass will not run.
[warning ]: DimensionMapping for %1615 = "air.Embedding"(%0, %1) {air.kEstimateToleranceLatency = -1.000000e+00 : f64, air.kEstimateToleranceUtilization = -1.000000e+00 : f64, air.kWeightGroupID = -1 : i64, kConfigured = true, kDoConvTiling = false, kDoRecompute = false, kInputsNamedDims = [["bertformaskedlm__bert__embeddings__token_type_embeddings__embedding_weight_dim0", "bertformaskedlm__bert__embeddings__word_embeddings__embedding_weight_dim1"], ["next_sentence_label_dim_0", "input_ids_dim_1"]], kInternalAddressesSlicing = false, kIsBubbledRecomputeNode = false, kMacConsumerNames = [["bertformaskedlm__bert__embeddings__add_t_input1"]], kNodeCategory = 1 : i64, kOutputsNamedDims = [["next_sentence_label_dim_0", "input_ids_dim_1", "bertformaskedlm__bert__embeddings__word_embeddings__embedding_weight_dim1"]]} : (tensor<2x1024xbf16>, tensor<256x128xi32>) -> tensor<256x128x1024xbf16> is not supported!
[info    ]: Building size named dims for graph hf_transformer_samba
[warning ]: Skip the process of updating air op's inputs/outputs size named dims.
[info    ]: Building nameddims for graph hf_transformer_samba
[warning ]: Skip the process of updating air op's inputs/outputs named dims.
[info    ]: Making Tensor Parallel Decisions for graph: hf_transformer_samba
[info    ]: Analyze tiling for graph: hf_transformer_samba
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__bert__pooler__dense__linear_t_input0
 inputs: {NamedTensor: shape {256, 1024} names: {next_sentence_label_dim_0, bertformaskedlm__bert__pooler__dense__weight_dim_1}}
 outputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__bert__pooler__dense__weight_dim_1, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__bert__pooler__dense__linear
 inputs: {NamedTensor: shape {1024, 1024} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, bertformaskedlm__bert__pooler__dense__linear_input_1_dim_1}, NamedTensor: shape {1024, 256} names: {bertformaskedlm__bert__pooler__dense__linear_input_1_dim_1, next_sentence_label_dim_0}, NamedTensor: shape {1024, 1} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__bert__pooler__activation__tanh
 inputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__cls__seq_relationship__linear
 inputs: {NamedTensor: shape {2, 1024} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, bertformaskedlm__cls__seq_relationship__linear_input_1_dim_1}, NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__linear_input_1_dim_1, next_sentence_label_dim_0}, NamedTensor: shape {2, 1} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {2, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__cls__seq_relationship__linear_t_output0
 inputs: {NamedTensor: shape {2, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {256, 2} names: {next_sentence_label_dim_0, bertformaskedlm__cls__seq_relationship__weight_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__crossentropyloss_1
 inputs: {NamedTensor: shape {256, 2} names: {next_sentence_label_dim_0, bertformaskedlm__crossentropyloss_1_num_classes}, NamedTensor: shape {256} names: {next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {256} names: {next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__crossentropyloss_swap_reshape
 inputs: {NamedTensor: shape {256, 128} names: {next_sentence_label_dim_0, labels_dim_1}}
 outputs: {NamedTensor: shape {32768} names: {bertformaskedlm__crossentropyloss_swap_reshape_output_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__crossentropyloss_swap_reshape_bwd
 inputs: {NamedTensor: shape {32768} names: {bertformaskedlm__crossentropyloss_swap_reshape_output_dim_0}}
 outputs: {NamedTensor: shape {256, 128} names: {next_sentence_label_dim_0, labels_dim_1}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__bert__pooler__dense__linear_t_input0_bwd
 inputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__bert__pooler__dense__weight_dim_1, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {256, 1024} names: {next_sentence_label_dim_0, bertformaskedlm__bert__pooler__dense__weight_dim_1}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__bert__pooler__dense__linear_bwd_loss
 inputs: {NamedTensor: shape {1024, 1024} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, bertformaskedlm__bert__pooler__dense__linear_input_1_dim_1}, NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__bert__pooler__dense__linear_input_1_dim_1, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__bert__pooler__activation__tanh_bwd
 inputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}, NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__cls__seq_relationship__linear_bwd_loss
 inputs: {NamedTensor: shape {2, 1024} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, bertformaskedlm__cls__seq_relationship__linear_input_1_dim_1}, NamedTensor: shape {2, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__linear_input_1_dim_1, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__cls__seq_relationship__linear_t_output0_bwd
 inputs: {NamedTensor: shape {256, 2} names: {next_sentence_label_dim_0, bertformaskedlm__cls__seq_relationship__weight_dim_0}}
 outputs: {NamedTensor: shape {2, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, next_sentence_label_dim_0}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__crossentropyloss_1_bwd_loss
 inputs: {NamedTensor: shape {256, 2} names: {next_sentence_label_dim_0, bertformaskedlm__crossentropyloss_1_num_classes}, NamedTensor: shape {256} names: {next_sentence_label_dim_0}, NamedTensor: shape {256} names: {next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {256, 2} names: {next_sentence_label_dim_0, bertformaskedlm__crossentropyloss_1_num_classes}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__cls__seq_relationship__linear_bwd_weight
 inputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__linear_input_1_dim_1, next_sentence_label_dim_0}, NamedTensor: shape {2, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {2, 1024} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, bertformaskedlm__cls__seq_relationship__linear_input_1_dim_1}, NamedTensor: shape {2, 1} names: {bertformaskedlm__cls__seq_relationship__weight_dim_0, bertformaskedlm__cls__seq_relationship__linear_bwd_weight_bias_output_size_1_dim}}
[warning ]: Try to squeeze a unsqueezable dim next_sentence_label_dim_0 for op bertformaskedlm__bert__pooler__dense__linear_bwd_weight
 inputs: {NamedTensor: shape {1024, 256} names: {bertformaskedlm__bert__pooler__dense__linear_input_1_dim_1, next_sentence_label_dim_0}, NamedTensor: shape {1024, 256} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, next_sentence_label_dim_0}}
 outputs: {NamedTensor: shape {1024, 1024} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, bertformaskedlm__bert__pooler__dense__linear_input_1_dim_1}, NamedTensor: shape {1024, 1} names: {bertformaskedlm__cls__seq_relationship__weight_dim_1, bertformaskedlm__bert__pooler__dense__linear_bwd_weight_bias_output_size_1_dim}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, input_ids_dim_1}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__0__attention__self__query__weight_dim_0, input_ids_dim_1}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__0__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, input_ids_dim_1}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__0__attention__self__key__weight_dim_0, input_ids_dim_1}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__0__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__0__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__0__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, input_ids_dim_1}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__0__attention__self__value__weight_dim_0, input_ids_dim_1}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__0__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__1__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__0__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__1__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__0__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__1__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__0__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__1__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__1__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__1__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__0__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__1__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__0__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__1__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__2__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__1__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__2__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__1__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__2__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__1__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__2__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__2__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__2__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__1__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__2__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__1__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__2__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__3__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__2__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__3__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__2__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__3__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__2__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__3__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__3__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__3__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__2__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__3__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__2__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__3__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__4__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__3__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__4__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__3__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__4__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__3__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__4__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__4__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__4__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__3__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__4__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__3__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__4__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__5__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__4__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__5__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__4__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__5__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__4__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__5__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__5__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__5__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__4__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__5__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__4__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__5__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__6__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__5__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__6__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__5__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__6__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__5__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__6__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__6__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__6__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__5__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__6__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__5__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__6__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__7__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__6__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__7__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__6__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__7__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__6__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__7__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__7__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__7__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__6__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__7__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__6__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__7__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__8__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__7__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__8__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__7__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__8__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__7__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__8__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__8__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__8__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__7__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__8__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__7__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__8__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__9__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__8__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__9__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__8__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__9__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__8__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__9__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__9__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__9__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__8__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__9__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__8__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__9__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__10__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__9__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__10__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__9__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__10__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__9__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__10__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__10__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__10__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__9__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__10__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__9__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__10__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__11__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__10__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__11__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__10__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__11__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__10__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__11__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__11__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__11__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__10__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__11__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__10__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__11__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__12__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__11__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__12__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__11__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__12__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__11__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__12__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__12__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__12__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__11__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__12__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__11__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__12__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__13__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__12__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__13__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__12__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__13__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__12__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__13__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__13__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__13__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__12__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__13__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__12__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__13__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__14__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__13__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__14__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__13__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__14__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__13__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__14__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__14__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__14__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__13__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__14__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__13__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__14__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__15__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__14__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__15__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__14__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__15__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__14__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__15__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__15__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__15__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__14__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__15__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__14__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__15__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__16__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__15__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__16__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__15__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__16__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__15__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__16__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__16__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__16__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__15__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__16__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__15__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__16__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__17__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__16__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__17__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__16__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__17__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__16__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__17__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__17__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__17__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__16__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__17__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__16__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__17__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__18__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__17__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__18__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__17__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__18__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__17__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__18__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__18__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__18__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__17__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__18__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__17__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__18__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__19__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__18__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__19__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__18__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__19__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__18__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__19__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__19__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__19__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__18__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__19__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__18__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__19__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__20__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__19__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__20__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__19__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__20__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__19__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__20__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__20__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__20__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__19__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__20__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__19__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__20__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__21__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__20__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__21__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__20__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__21__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__20__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__21__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__21__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__21__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__20__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__21__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__20__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__21__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__22__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__21__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__22__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__21__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__22__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__21__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__22__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__22__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__22__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__21__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__22__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__21__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__22__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__23__attention__self__query__weight_dim_0, bertformaskedlm__bert__encoder__layer__22__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__23__attention__self__key__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__22__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__23__attention__self__key__weight_dim_0, bertformaskedlm__bert__encoder__layer__22__attention__self__matmul_dim_row}}
[warning ]: Try to squeeze a unsqueezable dim bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_1 for op bertformaskedlm__bert__encoder__layer__23__attention__self__value__linear_t_output0_fused_reshape_bwd
 inputs: {NamedTensor: shape {16, 64, 128} names: {bertformaskedlm__bert__encoder__layer__23__attention__self__query__linear_t_output0_fused_reshape_output_dim_1, bertformaskedlm__bert__encoder__layer__23__attention__self__value__linear_t_output0_fused_reshape_output_dim_2, bertformaskedlm__bert__encoder__layer__22__attention__self__matmul_dim_row}}
 outputs: {NamedTensor: shape {1024, 128} names: {bertformaskedlm__bert__encoder__layer__23__attention__self__value__weight_dim_0, bertformaskedlm__bert__encoder__layer__22__attention__self__matmul_dim_row}}
[info    ]: Mapping for graph/function: hf_transformer_samba
[info    ]: Amortized resources overall: 4.500000e+02 PCUs, 6.890000e+02 PMUs, projected latency: 6.079219e-01 s, FLOPS: 1.216469e+02 T/s, DDR_BW: 6.463480e+01 GB/s
[info    ]: Legalizing node resources...
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 2: tensor<1024x1xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[warning ]: [LinearInputGradInfo] STOC Rounding enabled but the selected template does not support STOC Rounding
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x1xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1024x128xbf16>
[info    ]: Input 0: tensor<1024x128xbf16>
[info    ]: Input 1: tensor<1x128xbf16>
[info    ]: Lowering to TLIR succeeded.
[info    ]: Compilation succeeded.
[info    ]: Mac Compilation succeeded.
2024-04-08 23:46:10,495 - apps.nlp.transformers_on_rdu.transformers_hook - Process ID 1688339 - info     - NLP app finished
import blocksparse tasks from _NamespacePath(['/opt/sambaflow/apps/nlp/transformers_on_rdu/blocksparse/common/tasks'])
import blocksparse tasks from _NamespacePath(['/opt/sambaflow/apps/nlp/transformers_on_rdu/blocksparse/common/tasks'])
+ /opt/llvm11/bin/clang++ -DAT_PARALLEL_OPENMP=1 -D_THP_CORE -I/opt/llvm12/include -I./ -I/opt/sambaflow/include -fvisibility=hidden -fno-exceptions -fno-rtti -Werror -Wall -Wno-reorder -Wno-conversion -Wno-sign-conversion -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-variable -Wno-strict-aliasing -Wno-unused-local-typedefs -Wunreachable-code-aggressive -Wsometimes-uninitialized -Wconditional-uninitialized -Woverflow -Waddress-of-temporary -Wpointer-arith -Wimplicit -Wbitfield-enum-conversion -Wno-unused-private-field -DSOURCE_PREFIX_LENGTH=84 -g -D_GNU_SOURCE -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-unknown-pragmas -Wno-missing-braces -fopenmp=libomp -std=gnu++17 -o HfTransformerSamba.cpp.o -c HfTransformerSamba.cpp
+ /opt/llvm11/bin/clang++ -DAT_PARALLEL_OPENMP=1 -D_THP_CORE -I/opt/llvm12/include -I./ -I/opt/sambaflow/include -fvisibility=hidden -fno-exceptions -fno-rtti -Werror -Wall -Wno-reorder -Wno-conversion -Wno-sign-conversion -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-variable -Wno-strict-aliasing -Wno-unused-local-typedefs -Wunreachable-code-aggressive -Wsometimes-uninitialized -Wconditional-uninitialized -Woverflow -Waddress-of-temporary -Wpointer-arith -Wimplicit -Wbitfield-enum-conversion -Wno-unused-private-field -DSOURCE_PREFIX_LENGTH=84 -g -D_GNU_SOURCE -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-unknown-pragmas -Wno-missing-braces -fopenmp=libomp -std=gnu++17 -o TestHfTransformerSamba.cpp.o -c TestHfTransformerSamba.cpp
+ /opt/llvm11/bin/clang++ -I/opt/llvm12/include -I./ -I/opt/sambaflow/include -fvisibility=hidden -fno-exceptions -fno-rtti -Werror -Wall -Wno-reorder -Wno-conversion -Wno-sign-conversion -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-variable -Wno-strict-aliasing -Wno-unused-local-typedefs -Wunreachable-code-aggressive -Wsometimes-uninitialized -Wconditional-uninitialized -Woverflow -Waddress-of-temporary -Wpointer-arith -Wimplicit -Wbitfield-enum-conversion -Wno-unused-private-field -DSOURCE_PREFIX_LENGTH=84 -g TestHfTransformerSamba.cpp.o HfTransformerSamba.cpp.o -o test_arc_bertlrg -L/opt/llvm12/lib -L/opt/sambaflow/lib -L/opt/sambaflow/lib64 -L/usr/local/lib -L/usr/local/lib64 -L/usr/lib/x86_64-linux-gnu -L/usr/lib64 -Wl,-rpath,/opt/llvm12/lib:/opt/sambaflow/lib:/opt/sambaflow/lib64:/usr/local/lib:/usr/local/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib64 -lLLVMSupport -lMLIRDialect -lMLIROptLib -lMLIRPass -lMLIRQuant -lMLIRStandard -lMLIRIR -lMLIRRewrite -lMLIRPDL -lMLIRPDLInterp -lMLIRTensor -lMLIRParser -lMLIRAnalysis -lMLIRTransforms -lMLIRTransformUtils -lMLIRLoopAnalysis -lMLIRControlFlowInterfaces -lMLIRSideEffectInterfaces -lMLIRViewLikeInterface -lMLIRAffine -lMLIRLoopLikeInterface -lMLIRSCF -lMLIRCallInterfaces -lMLIRPresburger -lMLIRCopyOpInterface -lMLIREDSC -lMLIRLinalg -lTemplates -lPrismPlasmaTemplates -lPrismPlasma -lPrismShared -lpef -lyaml-cpp -lCompilerShared -lMLIRDynamicWrapper -lRAIL -lRAILUtil -lisl -lCompilerAppsCarraraTemplates -lTemplates -lBoxTemplates -lMacCompiler -lArcCompiler -lArchSpec -lomp -lpthread
+ set +x
[info    ]: Starting Plasma compile at /home/rainli/BertLarge/bertlrg/plasma_ir_modules/schedule_global
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_SymbolChipAssignment
[info    ]: [PASS] Running PlasmaIR003_LegalizeLocationAssignment
[info    ]: [PASS] Running PlasmaIR004_SymbolAttributePropagation
[info    ]: [PASS] Running PlasmaIR005_BuildDdrAndHostSegments
[info    ]: [PASS] Running PlasmaIR006_DdrAndHostSymbolAssignment
[info    ]: [PASS] Running PlasmaIR007_SegmentGarbageCollection
[info    ]: [PASS] Running PlasmaIR008_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR009_LegalizeDataAllocation
[info    ]: [PASS] Running PlasmaIR010_LegalizeSymbolOverlap
[info    ]: [PASS] Running PlasmaIR011_HbmLogical2Physical
[info    ]: [PASS] Running PlasmaIR012_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_0_0_.
[info    ]: Compilation succeeded for partition_0_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_1_0_.
[info    ]: Compilation succeeded for partition_1_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_2_0_.
[info    ]: Compilation succeeded for partition_2_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_3_0_.
[info    ]: Compilation succeeded for partition_3_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_4_0_.
[info    ]: Compilation succeeded for partition_4_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_5_0_.
[info    ]: Compilation succeeded for partition_5_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_6_0_.
[info    ]: Compilation succeeded for partition_6_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_7_0_.
[warning ]: Cannot predict latency of context HfTransformerSamba.partition_7_0_.naming_group_25_bertformaskedlm__crossentropyloss_1@kInputXVec_0. Please suppress the warning by explicitly annotate the latency with set_ctx_latency() in the TBufferContext
/scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/cross_entropy/rail/CrossEntropy.cpp:1514:0
tbuffer: partition_7_0_.naming_group_25_tbuf2a_7_0_35181 HfTransformerSamba.cpp:47221:0
[info    ]: Compilation succeeded for partition_7_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_8_0_.
[warning ]: Injecting transpose slotting counter to context kDefaultRead1 of partition_8_0_.naming_group_26_tbuf1a_8_0_35187.D_0_0_0 to read one vector per 4 cycles!  The hardware requires this transposed read has address monotonically increasing by the vector width within windows of 4 vectors. The bandwidth penalty won't be incurred if rewriting the read context to run for a multiple of 4 cycles and use read predication to mask off unwanted vectors!
/scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp:487:0
ctx: kDefaultRead1 /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp:487:0
pmu: D_0_0_0 /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:167:0
tbuffer: partition_8_0_.naming_group_26_tbuf1a_8_0_35187 HfTransformerSamba.cpp:47539:0
[warning ]: You have created a transposed access but are not respecting slotting rules so there will be a performance drop!
/scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp:487:0
ctx: kDefaultRead1 /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/compiler/rail/src/lib/node/tbuffer/TBufferContext.cpp:487:0
pmu: D_0_0_0 /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:167:0
tbuffer: partition_8_0_.naming_group_26_tbuf1a_8_0_35187 HfTransformerSamba.cpp:47539:0
[info    ]: Compilation succeeded for partition_8_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_9_0_.
[warning ]: Injecting transpose slotting counter to context kBackReadCtx of partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__0_bwd_weight_accum.tbuf_transpose_drain.D_0_0_0 to read one vector per 4 cycles!  The hardware requires this transposed read has address monotonically increasing by the vector width within windows of 4 vectors. The bandwidth penalty won't be incurred if rewriting the read context to run for a multiple of 4 cycles and use read predication to mask off unwanted vectors!
/scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
ctx: kBackReadCtx /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
pmu: D_0_0_0 /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:167:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__0_bwd_weight_accum.tbuf_transpose_drain /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1227:0
[warning ]: You have created a transposed access but are not respecting slotting rules so there will be a performance drop!
/scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
ctx: kBackReadCtx /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
pmu: D_0_0_0 /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:167:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__0_bwd_weight_accum.tbuf_transpose_drain /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1227:0
[warning ]: Injecting transpose slotting counter to context kBackReadCtx of partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__1_bwd_weight_accum.tbuf_transpose_drain.D_0_0_0 to read one vector per 4 cycles!  The hardware requires this transposed read has address monotonically increasing by the vector width within windows of 4 vectors. The bandwidth penalty won't be incurred if rewriting the read context to run for a multiple of 4 cycles and use read predication to mask off unwanted vectors!
/scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
ctx: kBackReadCtx /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
pmu: D_0_0_0 /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:167:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__1_bwd_weight_accum.tbuf_transpose_drain /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1227:0
[warning ]: You have created a transposed access but are not respecting slotting rules so there will be a performance drop!
/scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
ctx: kBackReadCtx /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
pmu: D_0_0_0 /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:167:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__1_bwd_weight_accum.tbuf_transpose_drain /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1227:0
[warning ]: Injecting transpose slotting counter to context kBackReadCtx of partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__2_bwd_weight_accum.tbuf_transpose_drain.D_0_0_0 to read one vector per 4 cycles!  The hardware requires this transposed read has address monotonically increasing by the vector width within windows of 4 vectors. The bandwidth penalty won't be incurred if rewriting the read context to run for a multiple of 4 cycles and use read predication to mask off unwanted vectors!
/scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
ctx: kBackReadCtx /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
pmu: D_0_0_0 /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:167:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__2_bwd_weight_accum.tbuf_transpose_drain /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1227:0
[warning ]: You have created a transposed access but are not respecting slotting rules so there will be a performance drop!
/scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
ctx: kBackReadCtx /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
pmu: D_0_0_0 /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:167:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__2_bwd_weight_accum.tbuf_transpose_drain /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1227:0
[warning ]: You have created a transposed access but are not respecting slotting rules so there will be a performance drop!
/scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
ctx: kBackReadCtx /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
pmu: D_0_0_0 /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/compiler/rail/src/lib/pass/transform/tbuffer/TBufferUnitElaborationPass.cpp:167:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__3_bwd_weight_accum.tbuf_transpose_drain /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1227:0
[warning ]: Cannot predict latency of context kBackReadCtx. Please suppress the warning by explicitly annotate the latency with set_ctx_latency() in the TBufferContext
/scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1234:0
tbuffer: partition_9_0_.HfTransformerSamba.partition_9_0_.naming_group_27_bertformaskedlm__cls__predictions__decoder__linear__gemm__3_bwd_weight_accum.tbuf_transpose_drain /scratch/jobs/33365192/debbuild/common/sambanova-arcprism-1.17.7-27/templates/src/templates/accumulator/rail/ParAccum.cpp:1227:0
[info    ]: Compilation succeeded for partition_9_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_10_0_.
[info    ]: Compilation succeeded for partition_10_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_11_0_.
[info    ]: Compilation succeeded for partition_11_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_12_0_.
[info    ]: Compilation succeeded for partition_12_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_13_0_.
[info    ]: Compilation succeeded for partition_13_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_14_0_.
[info    ]: Compilation succeeded for partition_14_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_15_0_.
[info    ]: Compilation succeeded for partition_15_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_16_0_.
[info    ]: Compilation succeeded for partition_16_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_17_0_.
[info    ]: Compilation succeeded for partition_17_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_18_0_.
[info    ]: Compilation succeeded for partition_18_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_19_0_.
[info    ]: Compilation succeeded for partition_19_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_20_0_.
[info    ]: Compilation succeeded for partition_20_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_21_0_.
[info    ]: Compilation succeeded for partition_21_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_22_0_.
[info    ]: Compilation succeeded for partition_22_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_23_0_.
[info    ]: Compilation succeeded for partition_23_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_24_0_.
[info    ]: Compilation succeeded for partition_24_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_25_0_.
[info    ]: Compilation succeeded for partition_25_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_26_0_.
[info    ]: Compilation succeeded for partition_26_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_27_0_.
[info    ]: Compilation succeeded for partition_27_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_28_0_.
[info    ]: Compilation succeeded for partition_28_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_29_0_.
[info    ]: Compilation succeeded for partition_29_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_30_0_.
[info    ]: Compilation succeeded for partition_30_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_31_0_.
[info    ]: Compilation succeeded for partition_31_0_
[info    ]: Starting Plasma compile at /home/rainli/BertLarge/bertlrg/plasma_ir_modules/sections_0_31
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ]: Starting Plasma compile at /home/rainli/BertLarge/bertlrg/plasma_ir_modules/schedule_0_31
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_SymbolChipAssignment
[info    ]: [PASS] Running PlasmaIR003_LegalizeLocationAssignment
[info    ]: [PASS] Running PlasmaIR004_LegalizeIncrementalDataAllocation
[info    ]: [PASS] Running PlasmaIR005_BuildDdrAndHostSegments
[info    ]: [PASS] Running PlasmaIR006_DdrAndHostSymbolAssignment
[info    ]: [PASS] Running PlasmaIR007_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR008_LegalizeDataAllocation
[info    ]: [PASS] Running PlasmaIR009_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_32_0_.
[info    ]: Compilation succeeded for partition_32_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_33_0_.
[info    ]: Compilation succeeded for partition_33_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_34_0_.
[info    ]: Compilation succeeded for partition_34_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_35_0_.
[info    ]: Compilation succeeded for partition_35_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_36_0_.
[info    ]: Compilation succeeded for partition_36_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_37_0_.
[info    ]: Compilation succeeded for partition_37_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_38_0_.
[info    ]: Compilation succeeded for partition_38_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_39_0_.
[info    ]: Compilation succeeded for partition_39_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_40_0_.
[info    ]: Compilation succeeded for partition_40_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_41_0_.
[info    ]: Compilation succeeded for partition_41_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_42_0_.
[info    ]: Compilation succeeded for partition_42_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_43_0_.
[info    ]: Compilation succeeded for partition_43_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_44_0_.
[info    ]: Compilation succeeded for partition_44_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_45_0_.
[info    ]: Compilation succeeded for partition_45_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_46_0_.
[info    ]: Compilation succeeded for partition_46_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_47_0_.
[info    ]: Compilation succeeded for partition_47_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_48_0_.
[info    ]: Compilation succeeded for partition_48_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_49_0_.
[info    ]: Compilation succeeded for partition_49_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_50_0_.
[info    ]: Compilation succeeded for partition_50_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_51_0_.
[info    ]: Compilation succeeded for partition_51_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_52_0_.
[info    ]: Compilation succeeded for partition_52_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_53_0_.
[info    ]: Compilation succeeded for partition_53_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_54_0_.
[info    ]: Compilation succeeded for partition_54_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_55_0_.
[info    ]: Compilation succeeded for partition_55_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_56_0_.
[info    ]: Compilation succeeded for partition_56_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_57_0_.
[info    ]: Compilation succeeded for partition_57_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_58_0_.
[info    ]: Compilation succeeded for partition_58_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_59_0_.
[info    ]: Compilation succeeded for partition_59_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_60_0_.
[info    ]: Compilation succeeded for partition_60_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_61_0_.
[info    ]: Compilation succeeded for partition_61_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_62_0_.
[info    ]: Compilation succeeded for partition_62_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_63_0_.
[info    ]: Compilation succeeded for partition_63_0_
[info    ]: Starting Plasma compile at /home/rainli/BertLarge/bertlrg/plasma_ir_modules/sections_32_63
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ]: Starting Plasma compile at /home/rainli/BertLarge/bertlrg/plasma_ir_modules/schedule_32_63
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_SymbolChipAssignment
[info    ]: [PASS] Running PlasmaIR003_LegalizeLocationAssignment
[info    ]: [PASS] Running PlasmaIR004_LegalizeIncrementalDataAllocation
[info    ]: [PASS] Running PlasmaIR005_BuildDdrAndHostSegments
[info    ]: [PASS] Running PlasmaIR006_DdrAndHostSymbolAssignment
[info    ]: [PASS] Running PlasmaIR007_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR008_LegalizeDataAllocation
[info    ]: [PASS] Running PlasmaIR009_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_64_0_.
[info    ]: Compilation succeeded for partition_64_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_65_0_.
[info    ]: Compilation succeeded for partition_65_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_66_0_.
[info    ]: Compilation succeeded for partition_66_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_67_0_.
[info    ]: Compilation succeeded for partition_67_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_68_0_.
[info    ]: Compilation succeeded for partition_68_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_69_0_.
[info    ]: Compilation succeeded for partition_69_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_70_0_.
[info    ]: Compilation succeeded for partition_70_0_
[info    ]: Logs are generated in /home/rainli/BertLarge/bertlrg///rail_gen//rail_compile_logs for partition_71_0_.
[info    ]: Compilation succeeded for partition_71_0_
[info    ]: Starting Plasma compile at /home/rainli/BertLarge/bertlrg/plasma_ir_modules/sections_64_71
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ]: Starting Plasma compile at /home/rainli/BertLarge/bertlrg/plasma_ir_modules/schedule_64_71
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_SymbolChipAssignment
[info    ]: [PASS] Running PlasmaIR003_LegalizeLocationAssignment
[info    ]: [PASS] Running PlasmaIR004_LegalizeIncrementalDataAllocation
[info    ]: [PASS] Running PlasmaIR005_BuildDdrAndHostSegments
[info    ]: [PASS] Running PlasmaIR006_DdrAndHostSymbolAssignment
[info    ]: [PASS] Running PlasmaIR007_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR008_LegalizeDataAllocation
[info    ]: [PASS] Running PlasmaIR009_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ]: Starting Plasma compile at /home/rainli/BertLarge/bertlrg/plasma_ir_modules/schedule_final
[info    ]: [PASS] Running PlasmaIR000_PrinterPass
[info    ]: [PASS] Running PlasmaIR001_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR002_PlasmaCanonicalizer
[info    ]: [PASS] Running PlasmaIR003_SegmentFinalization
[info    ]: [PASS] Running PlasmaIR004_LegalizeDataAllocation
[info    ]: [PASS] Running PlasmaIR005_LegalizeSymbolOverlap
[info    ]: [PASS] Running PlasmaIR006_PrinterPass
[info    ]: Plasma compile succeeded.
[info    ] : PEF file /home/rainli/BertLarge/bertlrg//bertlrg.pef created
COMPILE END AT 775
RUN
SHELL=/bin/bash
CONDA_EXE=/home/rainli/miniconda3/bin/conda
_CE_M=
PWD=/home/rainli/BertLarge
LOGNAME=rainli
CONDA_PREFIX=/home/rainli/miniconda3
OPENBLAS_NUM_THREADS=8
MOTD_SHOWN=pam
HOME=/home/rainli
LANG=en_US.UTF-8
VIRTUAL_ENV=/opt/sambaflow/apps/nlp/transformers_on_rdu/venv
CONDA_PROMPT_MODIFIER=(base) 
SSH_CONNECTION=140.221.69.48 55838 140.221.82.5 22
TERM=xterm-256color
_CE_CONDA=
LIBVIRT_DEFAULT_URI=qemu:///system
USER=rainli
CONDA_SHLVL=1
IBV_FORK_SAFE=1
SHLVL=2
SOFTWARE_HOME=/opt
CONDA_PYTHON_EXE=/home/rainli/miniconda3/bin/python
PS1=(venv) 
SSH_CLIENT=140.221.69.48 55838 22
CONDA_DEFAULT_ENV=base
SN_NUM_THREADS=32
OMP_NUM_THREADS=18
XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop
PATH=/opt/sambaflow/apps/nlp/transformers_on_rdu/venv/bin:/home/rainli/miniconda3/bin:/home/rainli/miniconda3/condabin:/opt/sambaflow/bin:/opt/sambaflow/venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/rainli/.local/bin:/home/rainli/bin
GLIBC_TUNABLES=glibc.cpu.hwcaps=-AVX_Usable,-AVX2_Usable,-Prefer_ERMS,-Prefer_FSRM,Prefer_No_AVX512,Prefer_No_VZEROUPPER,-AVX_Fast_Unaligned_Load,-ERMS
SSH_TTY=/dev/pts/2
OLDPWD=/home/rainli
_=/usr/bin/env
Submitted batch job 30854
Machine state After: 
Platform: DataScale SN30-8

Physical Inventory:
Component Name                        | Serial Number       | Inventory State | Functional State
------------------------------------------------------------------------------------------------
/NODE/XRDU_0/RDU_0                    | 205057B469B35895    | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_0/DIMM_A0    | 1F310A0             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_1/DIMM_B0    | 1F31085             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_2/DIMM_E0    | 1F3123D             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_3/DIMM_F0    | 1F310E6             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_4/DIMM_G0    | 1F6F66D             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_5/DIMM_H0    | 1F6F8AE             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_6/DIMM_C0    | 1F31058             | Present         | Online       
/NODE/XRDU_0/RDU_0/DDRCH_7/DIMM_D0    | 1F31185             | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1                    | 60504FB469B35895    | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_0/DIMM_J0    | 1F31211             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_1/DIMM_K0    | 1F312E9             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_2/DIMM_N0    | 1F312F6             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_3/DIMM_P0    | 1F31160             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_4/DIMM_Q0    | 1F3129B             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_5/DIMM_R0    | 1F31103             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_6/DIMM_L0    | 1F312FC             | Present         | Online       
/NODE/XRDU_0/RDU_1/DDRCH_7/DIMM_M0    | 1F3124F             | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_0/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_0/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0                    | 400804356D2D5895    | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_0/DIMM_A0    | 1F5BB53             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_1/DIMM_B0    | 1F5BD97             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_2/DIMM_E0    | 1F5BB36             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_3/DIMM_F0    | 1F5BB2D             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_4/DIMM_G0    | 1F5BD6E             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_5/DIMM_H0    | 1F5BB50             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_6/DIMM_C0    | 1F5BACD             | Present         | Online       
/NODE/XRDU_1/RDU_0/DDRCH_7/DIMM_D0    | 1F5BDEC             | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1                    | 605030356D2D5895    | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_0/DIMM_J0    | 1F5BB47             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_1/DIMM_K0    | 1F5BB41             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_2/DIMM_N0    | 1F5BB72             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_3/DIMM_P0    | 1F5BBDC             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_4/DIMM_Q0    | 1F5BB8B             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_5/DIMM_R0    | 1F5BCF1             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_6/DIMM_L0    | 1F5BB6C             | Present         | Online       
/NODE/XRDU_1/RDU_1/DDRCH_7/DIMM_M0    | 1F5BD33             | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_1/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_1/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0                    | 507076B16ABDB895    | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_0/DIMM_A0    | 1F5BD93             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_1/DIMM_B0    | 1F5BB10             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_2/DIMM_E0    | 1F5BB0F             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_3/DIMM_F0    | 1F5BD9A             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_4/DIMM_G0    | 1F5BD8D             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_5/DIMM_H0    | 1F5BD46             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_6/DIMM_C0    | 1F5BB1A             | Present         | Online       
/NODE/XRDU_2/RDU_0/DDRCH_7/DIMM_D0    | 1F5BB0E             | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1                    | 30702AB16ABDB895    | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_0/DIMM_J0    | 1F5BDC7             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_1/DIMM_K0    | 1F5BAF9             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_2/DIMM_N0    | 1F5BB08             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_3/DIMM_P0    | 1F5BDBA             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_4/DIMM_Q0    | 1F5BD99             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_5/DIMM_R0    | 1F5BB15             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_6/DIMM_L0    | 1F5BB2C             | Present         | Online       
/NODE/XRDU_2/RDU_1/DDRCH_7/DIMM_M0    | 1F5BB2B             | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_2/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_2/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0                    | 702867316ABDB895    | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_0/DIMM_A0    | 1F5BBE4             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_1/DIMM_B0    | 1F5BC28             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_2/DIMM_E0    | 1F5BCC6             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_3/DIMM_F0    | 1F5BDF2             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_4/DIMM_G0    | 1F5BAFE             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_5/DIMM_H0    | 1F5BAEC             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_6/DIMM_C0    | 1F5BBA7             | Present         | Online       
/NODE/XRDU_3/RDU_0/DDRCH_7/DIMM_D0    | 1F5BC96             | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_0/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1                    | 60084B316ABDB895    | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_0/DIMM_J0    | 1F5BDBE             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_1/DIMM_K0    | 1F5BC0D             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_2/DIMM_N0    | 1F5BC99             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_3/DIMM_P0    | 1F5BB68             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_4/DIMM_Q0    | 1F5BC21             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_5/DIMM_R0    | 1F5BC22             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_6/DIMM_L0    | 1F5BC9C             | Present         | Online       
/NODE/XRDU_3/RDU_1/DDRCH_7/DIMM_M0    | 1F5BC17             | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/PCIE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_0             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_1             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_2             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_3             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_4             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_5             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_6             | N/A                 | Present         | Online       
/NODE/XRDU_3/RDU_1/TILE_7             | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0                     | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_0              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_1              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_2              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_3              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_4              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_5              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_6              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_7              | N/A                 | Present         | Online       
/NODE/XRDU_3/SW_0/PORT_8              | N/A                 | Present         | Online       
/NODE/HOST/HIC_0/DPORT                | N/A                 | Present         | Online       
/NODE/HOST/HIC_1/DPORT                | N/A                 | Present         | Online       
/NODE/HOST/HIC_2/DPORT                | N/A                 | Present         | Online       
/NODE/HOST/HIC_3/DPORT                | N/A                 | Present         | Online
Duration:  776
